<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta lang="en">
    <title>orlando | didattica</title>
    <link href="/assets/images/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link href="/assets/css/main.css?1738496700829117000" rel="stylesheet">
    <meta name="description"   content="This is the official webpage of Gianluca Orlando, Assistant Professor in Mathematical Analysis at the Department of Mechanics, Mathematics & Management of Polytechnic of Bari. Gianluca Orlando's research focuses on Calculus of Variations, Gamma-convergence, semicontinuity and relaxation problems, applications to Fracture Mechanics (evolution for fracture, damage, plasticity, fatigue models) and Materials Science. He is also interested in the variational discrete-to-continuum analysis of spin systems (XY model, N-clock model, frustrated systems). Currently funded by Research for innovation - REFIN of Regione Puglia. 

    Questa è la pagina web ufficiale di Gianluca Orlando, Ricercatore a Tempo Determinato di tipo A in Analisi Matematica presso il Dipartimento di Meccanica, Matematica e Management (DMMM) del Politecnico di Bari (PoliBa). La ricerca di Gianluca Orlando è focalizzata sul calcolo delle variazioni, Gamma-convergenza, problemi di semicontinuità e rilassamento, applicazioni alla Meccanica delle Fratture (evoluzione per modelli di frattura, danno, plasticità, fatica) e Scienze dei Materiali. È anche interessato all'analisi variazionale dal discreto al continuo per sistemi di spin (modello XY, modello N-clock, sistemi frustrati). Al momento è finanziato dal programma Research for innovation - REFIN della Regione Puglia.">
    <meta name="keywords" content="Orlando, Gianluca, Politecnico di Bari, PoliBa, Dipartimento di Meccanica, Matematica e Management, DMMM, Analisi Matematica, didattica, ingegneria gestional, calcolo delle probabilità e statistica">

    <script src="/assets/js/scripts.js"></script>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;300;400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous">

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH" crossorigin="anonymous"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js" integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta name="google-site-verification" content="44tr-41mamfaxMm3K5fG48-GcmYgZDDBqX3aKtogsps" />

        
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>
    
</head>
  <body>
    <div class="wrapper">
        <div class="pageContainer">
            <div class="nameContainer">
                <div style="cursor: pointer;" onclick="window.location='/';">
                    gianluca orlando
                    </div>
            </div>
            <div class="navContainer">
                <div id="menuButton" class="navElement-menu" style="cursor: pointer;" onclick="menu();">
    menu
</div>

    <div style="cursor: pointer;" onclick="window.location='/cv.html';" class="navElement">
        cv
    </div>

    <div style="cursor: pointer;" onclick="window.location='/research.html';" class="navElement">
        research
    </div>

    <div style="cursor: pointer;" onclick="window.location='/didattica.html';" class="navElement">
        didattica
    </div>

            </div>
            <div class="defaultContainer">
            <h2 id="mathematical-methods-in-deep-learning">Mathematical Methods in Deep Learning</h2>
<h3 id="scudo">ScuDo</h3>

<hr />

<h3 id="mathematical-methods-in-deep-learning-1">Mathematical Methods in Deep Learning</h3>

<h4 id="repository">repository</h4>

<p>The repository with material of the course is available <a href="https://github.com/orlandopoliba/SCUDO-MMinDL">here</a>.</p>

<h4 id="program">program</h4>

<p>The objective of the course is to provide basic mathematical concepts and tools needed to have a clear insight into the mechanisms of neural networks and deep learning. Both theoretical and practical aspects will be covered, with hands-on exercise sessions for which the active participation of the students will be strongly encouraged.</p>

<p>The course will start with a general introduction to the objectives of data-driven machine learning and by explaining where neural networks and deep learning fit in this general framework. Then we will move on to the description of the basic building blocks of neural networks. We will introduce the concept of perceptron and of sigmoid neuron, and we will explain how these simple models can be used to build complex neural networks, with a gentle introduction to loss functions and the backpropagation algorithm. This will allow us to build a simple neural network that recognizes handwritten digits from the MNIST dataset with the aid of the PyTorch library from Python. With this concrete example in mind, we will discuss in detail some of the most important key aspects of neural networks.</p>

<p>We shall give an idea of the proof of the Universal Approximation Theorem, explaining how neural networks are able to approximate any continuous function. The proof will highlight the importance of the non-linearity in the activation functions of neural networks. Then we will refer to some results in the literature that show the benefits of using deep neural networks over shallow ones.</p>

<p>Next, we will move on to the choice of the loss function, discussing the choice of cross-entropy and log-likelihood loss functions. For this, we will recall some basic concepts of information theory and maximum likelihood estimation in statistics.</p>

<p>After the choice of the loss function, we will discuss the optimization of weights in neural networks. We will introduce, in a general setting, the algorithm of gradient descent, explaining how it is used to minimize functions. We will discuss pros and cons of the basic gradient descent algorithm. Then we will introduce in detail the algorithm used in practice to optimize the weights of neural networks, the stochastic gradient descent algorithm. Finally, we will discuss some of the most important variants of this algorithm, such as momentum update.</p>

<p>With these tools in hand, we will build a simple neural network from scratch.</p>

<p>We will move on to regularization techniques in neural networks, discussing the concepts of regularization and dropout. Moreover, we will discuss the problem of vanishing and exploding gradients in deep neural networks, and we will introduce the concept of batch normalization, explaining how it can be used to mitigate these problems.</p>

<p>If time permits, we will discuss some further topics in deep learning, such as convolutional neural networks for image recognition and natural language processing. A lecture will be dedicated to the question of how neural networks represent features in the data, discussing the concept of superposition of features and the Johnson-Lindenstrauss lemma.</p>

            </div>
        </div>
    </div>
  </body>
</html>